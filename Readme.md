# Domain Name Classification with Contextual Learning

This repository contains the code for training and evaluating models to classify domain names into two categories: Domain Generation Algorithms (DGAs) and normal domain names, using contextual learning techniques. The primary programming language used for development is Python.

## Introduction

Domain names play a crucial role in various internet-related applications. However, distinguishing between legitimate domain names and those generated by malicious algorithms (DGAs) can be challenging. This repository addresses this challenge by leveraging contextual learning methods, such as In-Context Learning (ICL) and Fine-Tuning, to develop accurate classifiers. These methods are explored and validated in our research to enhance cybersecurity measures against DGA-based attacks.

## Models

The primary models trained in this repository are Large Language Models (LLMs), such as LLama 3 8B. These models are trained on large datasets of domain names to learn contextual representations and patterns that distinguish between DGA-generated and normal domain names. The research demonstrates that fine-tuning these models with domain-specific data significantly improves detection capabilities, while ICL enables rapid adaptation to new threats.

## Results

The fine-tuned LLama 3 8B model achieved 94% accuracy and a 4% false positive rate, outperforming state-of-the-art models. However, due to its longer processing time (3.50 seconds), it is optimally used as a secondary verification layer in real-time cybersecurity systems.

## Usage

To use the models and evaluation scripts in this repository:

1. **Clone the repository to your local machine.**
    ```sh
    git clone https://github.com/reypapin/Domain-Name-Classification-with-Contextual-Learning.git
    cd Domain-Name-Classification-with-Contextual-Learning
    ```

2. **Install the necessary dependencies listed in `requirements.txt`.**
    ```sh
    chmod +x install.sh
    ./install.sh
    ```

3. **Run the provided script to start Ollama and the model:**
    ```python
    import os
    import threading
    import subprocess
    import requests
    import json

    def ollama():
        os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'
        os.environ['OLLAMA_ORIGINS'] = '*'
        subprocess.Popen(["ollama", "serve"])

    ollama_thread = threading.Thread(target=ollama)
    ollama_thread.start()

    subprocess.run(["ollama", "run", "llama3"])
    ```

## Evaluation

The repository includes scripts for evaluating the performance of the ICL and Fine-Tuning models. The results are documented and demonstrate the effectiveness of the proposed methods.

## Contributing

Contributions to this repository are welcome! If you have ideas for improvements, new features, or bug fixes, feel free to open an issue or submit a pull request.

1. **Fork the repository.**
2. **Create a new branch for your feature or bugfix.**
3. **Make your changes and commit them with descriptive messages.**
4. **Push your changes to your fork.**
5. **Submit a pull request to the main repository.**

## References

For more details on the research and methodologies used, please refer to the accompanying paper: *"Enhancing Cybersecurity: DGA Detection Using Large Language Models"* by Reynier Leyva La O et al.






